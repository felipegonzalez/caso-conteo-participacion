---
title: "Estimación de participación en el Conteo Rápido"
output: html_notebook
bibliography: 
 - "../referencias/referencias.bib"
 - "../referencias/paquetes.bib"
---


```{r}
library(cmdstanr)
library(posterior)
library(tidyverse)
library(patchwork)
source("../R/simular_resumenes.R")
```

### Problema

Para dar resultados en la noche de las elecciones, el INE diseña y selecciona
una muestra de casillas de las que se recogen los datos en cuanto la casilla
cierra. De esta manera no es necesario esperar a que todos los datos sean
registrados y validados para dar los resultados finales, lo cual puede tomar varios
días.

En este caso buscamos estimar la participación nacional 
(porcentaje de votantes registrados que emitió su voto) con los datos
de la muestra de casillas.

### Análisis conceptual

El diseño de la muestra es estratificado, con tamaño de muestra proporcional
al tamaño de cada estrato. Se dicidió muestrar 5% de las casillas de cada estrato.

Los estratos son principalmente dados por distritos electorales, aunque algunos
distritos más chicos se unen para tener un mínimo de casillas especificado:

- Los estratos son distritos electorales.
- Cada distrito electoral incluye varias casillas. 
- El tamaño de los estratos se mide con el número de casillas en todo el estrato.


Para este ejemplo **no** consideraremos las casillas especiales, que no tienen
una lista nominal definida. Aunque son una proporción considerablemente chica
de los votos, podríamos tratarlos más adelante de otra forma.


El marco muestral es el listado de todas las casillas. En este ejemplo usaremos
datos de un solo estado.

```{r}
load("../datos/marco_nal_2018.rda")
marco_tbl <- marco_nal_2018 %>% 
  filter(ln > 0) %>% 
  filter(id_estado == 9) %>%
  group_by(estrato) %>% 
  mutate(estrato_id = group_indices()) %>%
  arrange(estrato_id) %>% 
  ungroup %>% 
  mutate(casilla_id = row_number()) %>% 
  select(id, casilla_id, estrato_id, ln)
marco_tbl
```
Tenemos información de:

- Localización de la casilla (estrato, estado)
- Tamaño de la lista nominal en cada casilla.
- Si la casilla está en una zona rural o no.



Este es un problema de muestreo para población finita, y lo enfocamos 
de esa manera:

- Tenemos, para la muestra, número de votos que se emitieron y tamaño de la lista nominal ($y_i$, $n_i$), cuando $i\in S$
- A partir de estos datos, queremos estimar $y_i$ con $\hat{y}_i$ para $i\notin S$
- Estimaremos la participación como
$$\frac{\sum_{i\in S} y_i + \sum_{i\notin S} y_i}{\sum_i n_i}$$
Tenemos incertidumbre porque no observamos las $y_i$ fuera de la muestra.

Históricamente, las participación en elecciones presidenciales en México
ha oscilado entre el 40% y el 77% desde 1988 hasta 2020 (**referencia**)

Algunas casillas especiales no tiene lista nominal fija. Estas casillas serán
excluídas del cálculo, aunque quisiéramos reportar también estimaciones del 
total de votos
emitidos


### Espacio de observaciones

Las observaciones están dadas por conteos de las casillas en la 
muestra del número total de votos. 

```{stan, eval = FALSE}
data {
  // datos fijos
  int num_estratos;
  int n_est[num_estratos]; // num casillas por estrato 
  int N; // num casillas total
  int m_nominal[N]; // tamaño de lista nominal en cada casilla
  int est[N]; // indicador de estrato para cada casilla
  int n; // tamaño de muestra
  int muestra[n]; //indices de casillas en la muestra
  // observaciones
  int y_obs[n]; // conteos en cada casilla de votos totales
}
```

### Estadísticas resumen

Nuestras estadísticas resumen principales son:

- La participación sobre todas las casillas definido arriba
- Participación en cada estado.

### Desarrollo del modelo

Las observación para una casilla particular en la muestra es $y_i$, y suponemos
dada la lista nominal $n_i$.

En primer lugar, podríamos considerar que cada casilla $y_i$ es una observación
binomial con parámetros $(n_i, p_{est(i)})$, donde $p_{est(i)}$ es la probabilidad
de que un votante del estrato $i$ ejerza su voto. En este caso tendríamos que el valor esperado de votos en cada casilla es 
$n_ip_{est(i)}$ con $n_ip_{est(i)}(1-p_{est(i)})$


Pero esta hipótesis no es realmente creíble, pues:

1) Debe existir heterogeneidad en los votantes en cuanto a su probabilidad de voto,
dentro de las casillas.
2) Debe existir heterogeneidad en la probabilidad de que un votante vote a lo largo
de las casillas dentro los estratos. 

Esta heterogeneidad puede producir que la varianza de las observaciones dentro de cada
estrato está más (o menos) dispersa con respecto a lo que esperaríamos con el modelo
binomial. Por ejemplo, si en una casilla los votantes se dividen en unos que votan
con probabilidad cercana a 1 y otros cercana a 0, esperaríamos menos dispersión. 
Si las probabilidades de voto promedio en cada casilla son muy cercanas a la media del
estrato, esperaríamos dispersión similar a la de una binomial, pero si este no es el caso
podríamos observar sobredispersión con respecto a la binomial.


De manera que introducimos un nuevo parámetro $\sigma_{est(i)}$, y
ponemos

$$y_i \sim \mathsf{Normal} \left (n_ip_{est(i)}, \sigma_{est(i)}\sqrt{n_ip_{est(i)}(1-p_{est(i)})} \right)$$

Esto implica que si ponemos

$$p_i = y_i / {n}_i$$

entonces tenemos las observaciones

$$p_i \sim \mathsf{Normal} \left (p_{est(i)}, \sigma_{est(i)}\sqrt{\frac{p_{est(i)}(1-p_{est(i)})}{n_i}} \right)$$

y consideramos las observaciones $p_i$ de cada casilla independientes dado los parámetros
del estrato.

Ahora es necesario poner distribuciones iniciales. En primer lugar,
no creemos que las $p_{est(i)}$ sean independientes, pues dependen del interés
general de la población en cada elección particular.

$$p_{est(i)} \sim \mathsf{Beta}(\mu, \kappa)$$
Para la $\mu$, podemos usar datos históricos para establecer

$$\mu \sim \mathsf{Beta}(0.5, 10)$$

```{r}
mu_0 <- 0.5
kappa_0 <- 10
qbeta(c(0.01, 0.05, 0.5, 0.95, 0.99), mu_0*kappa_0, (1-mu_0)*kappa_0)
```

De modo que establecemos que es poco probable que la media sobre los
estratos sea menor a 40% o más grande de 78%, de acuerdo con la información
de elecciones anteriores.

Para la dispersión $\kappa$ usamos

$$\kappa \sim \mathsf{Gamma}(100, 10)$$

```{r}
m <- 10
cv <- 0.1
a <- (1/cv)^2
b <- a/m
c(a, b)
#a <- 1.5; b <- 0.02
cuantiles <- qgamma(c(0.01, 0.99), a, b)
cuantiles
```

Supongamos que $\mu = 0.6$. Entonces las posibilidades para las proporciones
de votos sobre los estratos son

```{r, message= FALSE}
p_1 <- qplot(rbeta(500, 0.17*cuantiles[1], (1-0.17)*cuantiles[1])) 
p_2 <- qplot(rbeta(500, 0.17*cuantiles[2], (1-0.17)*cuantiles[2]))
p_3 <- qplot(rbeta(500, 0.82*cuantiles[1], (1-0.82)*cuantiles[1]))
p_4 <- qplot(rbeta(500, 0.82*cuantiles[2], (1-0.82)*cuantiles[2]))
(p_1 + p_2) / (p_3 + p_4)
```
Consideramos todas estas combinaciones como casos límite de posibles
resultados, pero creemos que no son extremadamente improbables.

Ahora tenemos que poner una inicial para $\sigma_{est}$. Usaremos
$$\log\sigma_{est(i)} \sim N(0, \sigma)$$

Con $\sigma \sim N^+(0,0.5)$ 


Verificaremos simulando el ensemble bayesiano y calculando nuestras medidas resumen.

### Simular ensamble bayesiano


```{r}
set.seed(9913)
prop <- 0.05
## suponemos que la muestra se ha decidido
muestra_tbl <- marco_tbl %>%  group_by(estrato_id) %>% 
  mutate(seleccionada = rbinom(n(), 1, prop))
nrow(muestra_tbl %>% filter(seleccionada==1)) / nrow(muestra_tbl)
muestra <- muestra_tbl %>% filter(seleccionada==1) %>% 
  pull(casilla_id)
n <- length(muestra)
n
num_estratos <- length(unique(muestra_tbl$estrato_id))
num_estratos
##
datos_prueba <- list(casilla_id = marco_tbl$casilla_id, est = as.integer(marco_tbl$estrato_id),
                     num_estratos = num_estratos, N = nrow(marco_tbl), nom = marco_tbl$ln,
                     muestra = muestra, n = n)
jsonlite::write_json(datos_prueba, "../datos/datos_prueba.json")
sim_datos <- jsonlite::read_json("../datos/datos_prueba.json", simplifyVector = TRUE)
parametros <- jsonlite::read_json("../datos/datos_inicial.json", simplifyVector = TRUE)
print(parametros)
```


```{r}
sim_ensemble_datos <- c(sim_datos, parametros)
ruta <- file.path("../stan/ensemble_modelo.stan")
modelo_inicial <- cmdstan_model(ruta)
```


```{r}
ensemble <- simular_ensemble(modelo_inicial, sim_ensemble_datos, 1500)
```

```{r}
part_tbl <- ensemble$draws(c( "part_muestra", "mu", "kappa")) %>% as_draws_df()
ggplot(part_tbl, aes(x = 100 * part_muestra)) + geom_histogram(bins = 15)
```


Vemos que: 

- Las simulaciones de participación cubren un rango apropiado de posibilidades,
con más probabilidad entre 20% y 90%


Algunas simulaciones por estrato:

```{r}
sims_ejemplo_tbl <- ensemble$draws(c("y", "p_est")) %>% as_draws_df %>% 
  select(-.iteration, -.chain) %>% 
  pivot_longer(-.draw, names_to = "variable", values_to = "valor") %>% 
  separate(variable, sep = "[\\[\\]]", into = c("variable", "num", "c")) %>% 
  select(-c)
sims_ejemplo_tbl %>%
  filter(variable == "y") %>%
  mutate(estrato_id = sim_ensemble_datos$est[sim_ensemble_datos$muestra][as.integer(num)]) %>% 
  filter(.draw == 26) %>% 
ggplot(aes(x = factor(estrato_id), y = valor)) +
  geom_point() +
  facet_wrap(~.draw)
```



### Ajustar al ensemble simulado

Ahora veremos si podemos ajustar el modelo al ensemble simulado




```{r}
num_iter <- 26
ensemble$draws(c("mu", "kappa", "sigma")) %>%
as_draws_df() %>% 
  as_tibble() %>% filter(.draw == num_iter)
```


```{r}
sim_y <- ensemble$draws("y") %>% as_draws_df() %>% 
  filter(.draw == num_iter)  %>% 
  select(starts_with("y")) %>% 
  as.numeric()
```


```{r}
ruta <- file.path("../stan/modelo.stan")
modelo <- cmdstan_model(ruta)
```


```{r}
datos_sim <- c(sim_ensemble_datos, list("y" = sim_y))
ajuste <- modelo$sample(data = datos_sim,
                          seed = 22101,
                          iter_sampling = 1000, iter_warmup = 2000,
                          refresh = 1000, parallel_chains = 4,
                          show_messages = FALSE)
ajuste
```

```{r}
ajuste$cmdstan_diagnose()
```


Ahora examinamos las participaciones por estratos y nuestra estimación a total:

```{r}
calcular_estratos(ajuste, ensemble) %>% 
  ggplot(aes(x = estrato_num, ymin = part_0.05, y = part_est, ymax = part_0.95)) +
    geom_point(colour = "red", alpha = 0.5) +
    geom_linerange()
```

Y finalmente, nuestra estimación es

```{r}
estimador_final(ajuste)
```

### Calibración inferencial





### Ajuste a las observaciones

```{r}
remesa <- read_delim("../datos/REMESAS0100012200.txt", delim = "|", skip = 1) %>% 
  mutate(casilla = ifelse(TIPO_CASILLA %in% c("B", "C"), "B-C", TIPO_CASILLA)) %>% 
  unite("id", 
        c(iD_ESTADO, SECCION, ID_CASILLA, TIPO_CASILLA, EXT_CONTIGUA), remove = FALSE, sep = "-") %>% 
  filter(iD_ESTADO == 9)
muestra_obs <- left_join(remesa, marco_tbl) %>% 
  arrange(casilla_id)
datos_stan <- sim_ensemble_datos
y_obs <- pull(muestra_obs, TOTAL)
datos_stan$y <- y_obs %>% as.numeric
datos_stan$n <- length(y_obs)
datos_stan$muestra <- muestra_obs$casilla_id
```



```{r}
ajuste <- modelo$sample(data = datos_stan,
                          seed = 22101,
                          iter_sampling = 1000, iter_warmup = 2000,
                          refresh = 1000, parallel_chains = 4,
                          show_messages = FALSE)
ajuste
```

```{r}
ajuste$cmdstan_diagnose()
```

```{r}
ajuste
```

```{r}
prop_obs_tbl <- muestra_obs %>% 
  group_by(estrato_id) %>% 
  summarise(part_obs_muestra = sum(TOTAL) / sum(LISTA_NOMINAL)) %>% 
  mutate(estrato_num = as.character(estrato_id))
calcular_estratos(ajuste) %>% 
  left_join(prop_obs_tbl) %>% 
  ggplot(aes(x = estrato_num, ymin = part_0.05, y = part_obs_muestra, ymax = part_0.95)) +
  geom_point(colour = "red", size = 2) +
  geom_linerange()
```

```{r}
estimador_final(ajuste)
```

En general observamos sobredispersión con respecto al modelo binomial:

```{r}
media_exp <- function(x) mean(exp(x))
ajuste$draws("log_sigma_est") %>% as_draws_df() %>% 
  select(starts_with("log_sigma")) %>% 
  summarise(across(everything(), media_exp)) %>% 
  pivot_longer(everything())
```



### Verificación posterior dentro de muestra




### Siguientes pasos

- Calibración inferencial
- Agregar casillas especiales (que no tienen lista nominal)
- Verificación posterior a nivel casilla

### Conclusiones

